{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2 (Ubuntu Linux)","language":"python","metadata":{"cocalc":{"description":"Python 2 programming language","priority":5,"url":"https://www.python.org/"}},"name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15rc1"},"colab":{"name":"3. Extraccion Features.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"2NOsUDmKA76J","colab_type":"text"},"source":["## CountVectorizer\n","\n","CountVectorizer convierte un conjunto de documentos a vectores para que podamos aplicarlos en ciertos modelos numericos. Básicamente su funcion consiste en contar el número de veces que una palabra en particular ha ocurrido.\n"]},{"cell_type":"code","metadata":{"id":"siZ_hRV_A76N","colab_type":"code","outputId":"e27f6116-2361-46ab-910b-8a9a2c0bdc46","executionInfo":{"status":"ok","timestamp":1584722455395,"user_tz":180,"elapsed":1323,"user":{"displayName":"Billy Mark","photoUrl":"","userId":"02961098580051798310"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","docs = [\"IA la lleva.\", \"IA rock! wohooo!\", \"Mi nombre es IA, y soy un Pythonista!\"]\n","cv = CountVectorizer()\n","X = cv.fit_transform(docs)\n","print(X.todense())\n","print(cv.vocabulary_)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[[0 1 1 1 0 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 1 0 0 1]\n"," [1 1 0 0 1 1 1 0 1 1 0]]\n","{u'ia': 1, u'pythonista': 6, u'lleva': 3, u'la': 2, u'mi': 4, u'wohooo': 10, u'un': 9, u'rock': 7, u'soy': 8, u'nombre': 5, u'es': 0}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"qp2fSWHZA76T","colab_type":"text"},"source":["## DictVectorizer\n","\n","DictVectorizer convierte mappings a vectores. \n","\n"]},{"cell_type":"code","metadata":{"id":"AkdaOuc1A76V","colab_type":"code","outputId":"16125329-eab2-45af-e18e-8cb9ef98a5b4","executionInfo":{"status":"ok","timestamp":1584722592626,"user_tz":180,"elapsed":834,"user":{"displayName":"Billy Mark","photoUrl":"","userId":"02961098580051798310"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from sklearn.feature_extraction import DictVectorizer\n","\n","docs = [{\"IA\": 1, \"es\": 1, \"bakan\": 2}, {\"Yo\": 1, \"no\": 1, \"se\": 2, \"si\": 3, \"me\": 1, \"olvidaste\": 2, \"lala\": 3}]\n","dv = DictVectorizer()\n","X = dv.fit_transform(docs)\n","print(X.todense())\n","print(dv.vocabulary_)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[[1. 0. 2. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 3. 1. 1. 2. 2. 3.]]\n","{'me': 5, 'Yo': 1, 'no': 6, 'lala': 4, 'si': 9, 'olvidaste': 7, 'se': 8, 'IA': 0, 'bakan': 2, 'es': 3}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"bH3AmvilA76Z","colab_type":"text"},"source":["## TfidfVectorizer\n","\n","En aplicaciones de _Text-Mining_ podemos requerir convertir el texto en vectores para usar algoritmos de machine learning. En general esto significa usar el espacio vectorial. Si bien CountVectorizer podría ser una solución, palabras como \"el\", \"una\", \"o\" etc. son palabras altamente frecuentes es decir se usan a menudo en todo tipo de documentos, por lo cual el uso de CountVectorizer le dará más énfasis a tales recuentos de palabras las cuales resultan no ser __relevantes__. Podríamos evitar estos problemas utilizando `stop_words =\" palabra-comun \"` que filtraría las palabras comunes pero puede suceder que en un contexto especifico tenga un vocabulario diferente. Por ejemplo, una conversación entre 2 estudiantes de Machine Learning tendría palabras como \"RAM\", \"procesador\", \"GPU\", las cuales se mencionar con demasiada frecuencia, luego tendrá que agregar manualmente tales stop-words. Ello deberia realizarse para cada problema que resuelva.\n","\n","En tales escenarios, se recomienda usar `TfidfVectorizer` que se encargará de tales cosas. Cada palabra recibe un número de acuerdo con la siguiente fórmula:\n","\n","$$ \\text{tfidf }\\left(\\text{word}\\right)=\\text{tf}\\left(\\text{word},\\text{document}_i\\right)\\cdot\\text{idf}\\left(\\text{word}\\right) $$\n","\n","Donde:\n","\n","1. tf (word, document_i) = Término de frecuencia de una palabra en el documento específico i.\n","2. idf (palabra) = Frecuencia inversa de documento de la palabra.\n","\n","La frecuencia inversa de documentos se define como el logaritmo del ratio entre el número de documentos y el número de veces que la palabra ha aparecido en cualquier documento.\n","\n","$$ \\text{idf }\\left(w\\right)=\\log\\left(\\frac{n_d}{df\\left(w\\right)}\\right)$$\n","\n","Donde:\n","1. df(w) = número de veces que la palabra ha aparecido en cualquier documento.\n","\n","De manera intuitiva, si una palabra ha aparecido demasiadas veces en otro documento (como palabras comunes \"el\", \"es\"), entonces le da menos importancia a tales palabras en contraste con las palabras que han aparecido más veces en un solo documento respecto a otras. Esto significa que si una palabra en particular aparece más veces en un solo documento, entonces podría ser una característica importante.\n","\n","Tenga en cuenta que el numerador y el denominador se agregan con `1` para evitar el underflow, por ejemplo. cuando la frecuencia del documento es 0\n","\n","Además, Sklearn también normaliza la salida de tfidf para que tenga una norma de 1. Esto es importante ya que nos interesan las similitudes, por lo que los vectores como (1, 1) y (3, 3) son realmente iguales (van en la misma dirección, solo tiene diferentes pesos) lo cual se logra dividiendo por la longitud del vector.\n","\n","$$v_i=\\frac{v_i}{\\left|v\\right|_2}=\\frac{v_i}{\\sqrt{v_1^2+v_2^2+v_3^2+....+v_n^2}}$$\n"]},{"cell_type":"code","metadata":{"id":"EO45rKesA76a","colab_type":"code","outputId":"4cc40a5c-e3cf-4ba4-d7eb-c50f012bdf18","executionInfo":{"status":"ok","timestamp":1584722780714,"user_tz":180,"elapsed":963,"user":{"displayName":"Billy Mark","photoUrl":"","userId":"02961098580051798310"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","cv_vectorizer = CountVectorizer()\n","docs = [\"IA es un guitarrista\", \"IA es un musico\", \"IA es tambien un programador feliz\"]\n","X_idf = tfidf_vectorizer.fit_transform(docs)\n","X_cv = cv_vectorizer.fit_transform(docs)\n","print(X_idf.todense())\n","print(tfidf_vectorizer.vocabulary_)\n","print(X_cv.todense())\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[[0.41285857 0.         0.69903033 0.41285857 0.         0.\n","  0.         0.41285857]\n"," [0.41285857 0.         0.         0.41285857 0.69903033 0.\n","  0.         0.41285857]\n"," [0.29360705 0.49711994 0.         0.29360705 0.         0.49711994\n","  0.49711994 0.29360705]]\n","{u'feliz': 1, u'tambien': 6, u'un': 7, u'guitarrista': 2, u'ia': 3, u'musico': 4, u'programador': 5, u'es': 0}\n","[[1 0 1 1 0 0 0 1]\n"," [1 0 0 1 1 0 0 1]\n"," [1 1 0 1 0 1 1 1]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"lo_88pOnA76e","colab_type":"text"},"source":["Podemos observar que \"IA\" y \"es\" tienen menor peso que \"guitarrista\", \"musico\", \"programador\""]},{"cell_type":"code","metadata":{"id":"oIbZTe1mA76f","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}